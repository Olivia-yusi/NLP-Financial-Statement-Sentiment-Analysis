{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/oliviali/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/oliviali/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/oliviali/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import dok_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self, feature_method, min_feature_ct=1, L2_reg=1.0):\n",
    "        \"\"\"\n",
    "        :param feature_method: featurize function\n",
    "        :param min_feature_count: int, ignore the features that appear less than this number to avoid overfitting\n",
    "        \"\"\"\n",
    "        self.feature_vocab = {}\n",
    "        self.feature_method = feature_method\n",
    "        self.min_feature_ct = min_feature_ct\n",
    "        self.L2_reg = L2_reg\n",
    "\n",
    "    def featurize(self, X):\n",
    "        \"\"\"\n",
    "        # Featurize input text\n",
    "\n",
    "        :param X: list of texts\n",
    "        :return: list of featurized vectors\n",
    "        \"\"\"\n",
    "        featurized_data = []\n",
    "        for text in X:\n",
    "            feats = self.feature_method(text)\n",
    "            featurized_data.append(feats)\n",
    "        return featurized_data\n",
    "\n",
    "    def pipeline(self, X, training=False):\n",
    "        \"\"\"\n",
    "        Data processing pipeline to translate raw data input into sparse vectors\n",
    "        :param X: featurized input\n",
    "        :return: 2d sparse vectors\n",
    "        \n",
    "        Implement the pipeline method that translate the dictionary like feature vectors into homogeneous numerical\n",
    "        vectors, for example:\n",
    "        [{\"fea1\": 1, \"fea2\": 2}, \n",
    "         {\"fea2\": 2, \"fea3\": 3}] \n",
    "         --> \n",
    "         [[1, 2, 0], \n",
    "          [0, 2, 3]]\n",
    "          \n",
    "        Hints:\n",
    "        1. How can you know the length of the feature vector?\n",
    "        2. When should you use sparse matrix?\n",
    "        3. Have you treated non-seen features properly?\n",
    "        4. Should you treat training and testing data differently?\n",
    "        \"\"\"\n",
    "        # Have to build feature_vocab during training\n",
    "        if training:\n",
    "            for dic in X:\n",
    "                for key,value in dic.items():\n",
    "                    if key in self.feature_vocab:\n",
    "                        self.feature_vocab[key] += value\n",
    "                    else:\n",
    "                        self.feature_vocab[key] = value\n",
    "            \n",
    "            pos = [\"non_seen\"] + [key for key,value in self.feature_vocab.items() if value >= self.min_feature_ct]\n",
    "            \n",
    "            self.feature_vocab = {\"non_seen\":0}\n",
    "            for idx,value in enumerate(pos):\n",
    "                self.feature_vocab[value] = idx\n",
    "            \n",
    "            \n",
    "            \n",
    "        # Translate raw texts into vectors\n",
    "        n_feature_vector = len(self.feature_vocab)\n",
    "        n_sample = len(X)\n",
    "        sp_matrix = dok_matrix((n_sample,n_feature_vector),dtype = \"int64\")\n",
    "\n",
    "        for i in range(n_sample):\n",
    "            for key in X[i].keys():\n",
    "                if key in self.feature_vocab:\n",
    "                    sp_matrix[i,self.feature_vocab[key]] = X[i][key]\n",
    "                else:\n",
    "                    sp_matrix[i,self.feature_vocab[\"non_seen\"]] = X[i][key]\n",
    "\n",
    "        return sp_matrix\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = self.pipeline(self.featurize(X), training=True)\n",
    "\n",
    "        D, F = X.shape\n",
    "        self.model = LogisticRegression(C=self.L2_reg)\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.pipeline(self.featurize(X))\n",
    "        return self.model.predict(X)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        X = self.pipeline(self.featurize(X))\n",
    "        return self.model.score(X, y)\n",
    "\n",
    "    # Write learned parameters to file\n",
    "    def save_weights(self, filename='weights.csv'):\n",
    "        weights = [[\"__intercept__\", self.model.intercept_[0]]]\n",
    "        \n",
    "        for feat, idx in self.feature_vocab.items():\n",
    "            weights.append([feat, self.model.coef_[0][idx]])\n",
    "        \n",
    "        weights = pd.DataFrame(weights)\n",
    "        weights.to_csv(filename, header=False, index=False)\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from file\n",
    "from sklearn.utils import Bunch\n",
    "import numpy as np\n",
    "\n",
    "def load_data(filename):\n",
    "    \"\"\"\n",
    "    Load input data and return sklearn.utils.Bunch \n",
    "    \"\"\"\n",
    "    target, text = [], []\n",
    "    with open(filename, encoding=\"utf8\", errors=\"ignore\") as file:\n",
    "        for line in file:\n",
    "            cols = line.split(\"@\")\n",
    "            text.append(cols[0].rstrip()) \n",
    "            \n",
    "            if cols[1] == 'positive\\n':\n",
    "                y_label = 1\n",
    "            elif (cols[1] == 'negative\\n') | (cols[1] == 'negative'):\n",
    "                y_label = -1\n",
    "            elif cols[1] == 'neutral\\n':\n",
    "                y_label = 0\n",
    "            else:\n",
    "                raise('Error')\n",
    "            target.append(y_label)\n",
    "\n",
    "    return Bunch(text=text, target=np.array(target))\n",
    "def save_prediction(arr, filename=\"prediction.csv\"):\n",
    "    \"\"\"\n",
    "    Save the prediction into file\n",
    "    \"\"\"\n",
    "    out = open(filename, \"w\", encoding=\"utf8\")\n",
    "    for idx, val in enumerate(arr):\n",
    "        if val == 1:\n",
    "            pred = 'positive'\n",
    "        elif val == -1:\n",
    "            pred = 'negative'\n",
    "        \n",
    "        out.write(\"%s,%s\\n\" % (idx, pred))\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(\"/Users/oliviali/Downloads/Sentences_66Agree.txt\")\n",
    "X, y = data.text, data.target\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_featurize(text):\n",
    "    \"\"\"\n",
    "    !! Do not work on this yet, work on the model and come back later !!\n",
    "    \n",
    "    Write your own code below\n",
    "    \"\"\"\n",
    "    from nltk.tokenize import word_tokenize\n",
    "       \n",
    "    \n",
    "    porter = nltk.PorterStemmer()\n",
    "\n",
    "    \n",
    "    from nltk.corpus import stopwords \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "    ## Bag of words\n",
    "    feats = {}\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    \n",
    "    words = [porter.stem(x) for x in words]\n",
    "    \n",
    "    words = [x for x in words if not x in stop_words] \n",
    "    \n",
    "        \n",
    "    for k in enumerate(set(words)):\n",
    "        \n",
    "        if k not in feats:\n",
    "            feats[k] = 1\n",
    "        else:\n",
    "            feats[k] += 1\n",
    "            \n",
    "            \n",
    "    return feats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:  0.7516096238563199\n",
      "Dev set accuracy:  0.6595576619273301\n"
     ]
    }
   ],
   "source": [
    "#test bag of words\n",
    "cls = SentimentClassifier(feature_method=bag_of_words_featurize, min_feature_ct = 10)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set accuracy: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS_featurize(text):\n",
    "    \"\"\"\n",
    "    !! Do not work on this yet, work on the model and come back later !!\n",
    "    \n",
    "    Write your own code below\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    \n",
    "    from nltk.corpus import stopwords \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "\n",
    "    feats = {}\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    words = [wnl.lemmatize(x) for x in words]\n",
    "    \n",
    "    words = [x for x in words if not x in stop_words] \n",
    "    \n",
    "    default_tagger = nltk.DefaultTagger('NN')\n",
    "    \n",
    "    words = default_tagger.tag(words)\n",
    "\n",
    "    feats = {}   \n",
    "    for k in enumerate(set(words)):\n",
    "        \n",
    "        if k not in feats:\n",
    "            feats[k] = 1\n",
    "        else:\n",
    "            feats[k] += 1\n",
    "           \n",
    "            \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy:  0.6699423924093527\n",
      "Dev set accuracy:  0.665086887835703\n"
     ]
    }
   ],
   "source": [
    "#test POS\n",
    "cls = SentimentClassifier(feature_method=POS_featurize, min_feature_ct = 10, L2_reg = 0.1)\n",
    "cls = cls.fit(X_train, y_train)\n",
    "print(\"Training set accuracy: \", cls.score(X_train, y_train))\n",
    "print(\"Dev set accuracy: \", cls.score(X_dev, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
